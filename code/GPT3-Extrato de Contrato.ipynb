{"cells":[{"cell_type":"markdown","metadata":{"id":"v0vDg4nue4vO"},"source":["## OpenAI Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4263,"status":"ok","timestamp":1667689468811,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"E9chkLJcd226"},"outputs":[],"source":["!pip -q install openai"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1667689468812,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"9853RCHId6-2"},"outputs":[],"source":["import json\n","import openai\n","import uuid\n","import numpy as np\n","\n","#openai.api_key =  # isert valid GPT key for use"]},{"cell_type":"markdown","metadata":{"id":"WyOWOrJDUOHF"},"source":["## Example Class"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1667689468815,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"vvtSoikor1VV"},"outputs":[],"source":["class Example:\n","\n","   # Stores an input, output pair and formats it to prime the model\n","   def __init__(self, input, output):\n","       self.input = input\n","       self.output = output\n","       self.id = uuid.uuid4().hex\n","\n","   # To obtain the input provided for an example\n","   def get_input(self):\n","       return self.input\n","\n","   # To obtain the output provided for an example\n","   def get_output(self):\n","       return self.output\n","\n","   # To obtain the unique id of an example\n","   def get_id(self):\n","       return self.id"]},{"cell_type":"markdown","metadata":{"id":"G9Ywvp_5iW0Y"},"source":["## GPT3 Class"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1667689468816,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"lui8ukC7iUlB"},"outputs":[],"source":["class GPT3:\n","\n","   \"\"\"\n","      Params engine: Model to be used. Options are Davinci, Babbage, Ada and Curie.\n","      temperature: Amount of randomness to be introduced in the predictions of the\n","      model Setting a higher value of temperature would be useful for creative \n","      applications whereas a lower value will be suitable for well defined answers.\n","      max_tokens: Maximum length of number of tokens accepted by the prompt.\n","   \"\"\"\n","\n","   # initialises parameters and adds default values\n","   def __init__(self, engine='davinci', temperature=0.5, max_tokens=100,\n","\n","       input_prefix=\"input: \", input_suffix=\"\\n\", output_prefix=\"output: \",\n","       output_suffix=\"\\n\\n\", append_output_prefix_to_query=False):\n","       self.examples = {}\n","       self.engine = engine\n","       self.temperature = temperature\n","       self.max_tokens = max_tokens\n","       self.input_prefix = input_prefix\n","       self.input_suffix = input_suffix\n","       self.output_prefix = output_prefix\n","       self.output_suffix = output_suffix\n","       self.append_output_prefix_to_query = append_output_prefix_to_query\n","       self.stop = (output_suffix + input_prefix).strip()\n","\n","   # Adds an example to the model object. Example is an instance of the Example class.\n","   def add_example(self, ex):\n","       assert isinstance(ex, Example), \"Please create an Example object.\"\n","       self.examples[ex.get_id()] = ex\n","\n","   # Converts all the examples to a particular format to prime the model.\n","   def get_prime_text(self):\n","       return \"\".join(\n","           [self.format_example(ex) for ex in self.examples.values()])\n","\n","   # Creates a query for the API request\n","   def craft_query(self, prompt):\n","       q = self.get_prime_text(\n","       ) + self.input_prefix + prompt + self.input_suffix\n","\n","       if self.append_output_prefix_to_query:\n","           q = q + self.output_prefix\n","       return q\n","\n","   # Calls the API using the Completion endpoint with the specified values of the parameters\n","   def submit_request(self, prompt):\n","       response = openai.Completion.create(engine=self.engine,\n","                                           prompt=self.craft_query(prompt),\n","                                           max_tokens=self.max_tokens,\n","                                           temperature=self.temperature,\n","                                           top_p=1,\n","                                           n=1,\n","                                           stream=False,\n","                                           stop=self.stop)\n","       return response\n","\n","   # Formats the input output pair with appropriate prefixes and suffixes\n","   def format_example(self, ex):\n","       return self.input_prefix + ex.get_input(\n","       ) + self.input_suffix + self.output_prefix + ex.get_output(\n","       ) + self.output_suffix"]},{"cell_type":"markdown","metadata":{"id":"sNXgdydNSuqR"},"source":["# Atos de Extrato de Contrato"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":876,"status":"ok","timestamp":1667689469670,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"SYbDKtX3YlSh"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1935,"status":"ok","timestamp":1667689471592,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"fbWTVBT3Wrx2","outputId":"500cd6d8-0b07-4d77-cc3a-84784584128b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1667689471593,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"0X0FSxaWW4rq","outputId":"8bf3e38f-7c7b-42e8-e2fd-60ea83147cda"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive/MyDrive/Datasets/dodf_atos_pessoal_v3.csv\n","drive/MyDrive/Datasets/dodf_atos_pessoal_v4.csv\n","drive/MyDrive/Datasets/DODFCorpus_contratos_licitacoes_v1.csv\n"]}],"source":["!ls  drive/MyDrive/Datasets/*csv "]},{"cell_type":"markdown","metadata":{"id":"KYhxNjYJYe9u"},"source":["### Pré-processamento"]},{"cell_type":"markdown","metadata":{"id":"HydT5DYqpchO"},"source":["Acesso ao corpus de Contratos e Licitações:"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1667689472002,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"3CUbpUGzYiLy"},"outputs":[],"source":["df = pd.read_csv('drive/MyDrive/Datasets/DODFCorpus_contratos_licitacoes_v1.csv')"]},{"cell_type":"markdown","metadata":{"id":"aV7fei0MpSmq"},"source":["Listando tipos de ato presentes no corpus:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1667689472003,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"0ePY1V-ehtSF","outputId":"f904811d-6723-407d-d9f0-b8ea5551e905"},"outputs":[{"data":{"text/plain":["array(['REL_AVISO_LICITACAO', 'REL_SUSPENSAO_LICITACAO',\n","       'REL_EXTRATO_CONTRATO', 'REL_ADITAMENTO_CONTRATO',\n","       'REL_EXTRATO_CONVENIO', 'REL_ANUL_REVOG_LICITACAO'], dtype=object)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tipos_ato = df.tipo_rel.unique()\n","tipos_ato"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1667689472004,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"9645KzCMp18b"},"outputs":[],"source":["df_extrato_contrato_label = df[df.tipo_rel=='REL_EXTRATO_CONTRATO']\n","df_extrato_contrato = df_extrato_contrato_label[df_extrato_contrato_label.tipo_ent=='EXTRATO_CONTRATO'].copy()"]},{"cell_type":"markdown","metadata":{"id":"JnkcOoyGkrcR"},"source":["Transformando anotação em um dicionário..."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":476,"status":"ok","timestamp":1667689472473,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"EjHMUZs3rsd8"},"outputs":[],"source":["labels = []\n","for name, group in  df_extrato_contrato_label.groupby('id_ato', sort=False):\n","    d = {}\n","    for ent, txt in zip(group.tipo_ent, group.texto):\n","        if ent.isupper():\n","             continue\n","        if ent not in d:\n","            d[ent] = [txt]\n","        else:\n","            d[ent].append(txt)\n","    labels.append(d)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1667689472474,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"e91eWYU6RY8d","outputId":"698ea2af-6987-4736-df87-dcaac9679d6e"},"outputs":[{"data":{"text/plain":["{'numero_contrato': ['45/2018'],\n"," 'processo_gdf': ['00070-00016600/2018-76'],\n"," 'orgao_contratante': ['SEAGRI/DF'],\n"," 'entidade_contratada': ['CIMAG  COMERCIO DE\\nIMPLEMENTOS E MAQUINAS AGRICOLAS LTDA'],\n"," 'valor_contrato': ['39.600,00'],\n"," 'nota_empenho': ['2018NE00463', '2018NE00464'],\n"," 'unidade_orcamentaria': ['14101'],\n"," 'programa_trabalho': ['20.606.6207.2889.0003'],\n"," 'natureza_despesa': ['449052'],\n"," 'fonte_recurso': ['732014482', '100000000'],\n"," 'data_assinatura_contrato': ['21/11/2018'],\n"," 'objeto_contrato': ['O Contrato tem por objeto a aquisicao\\ndos seguintes itens: Item 20: Quantidade 03 ( tres). GRADE ARADORA CONTROLE R E M O TO ,\\ncom 14 discos de 26 polegadas de diametro e 06 mm de espessura minima, mancais de rolamento\\ncom lubrificacao permanente em banho de oleo, ou a graxa, espacamento minimo entre os discos de\\n230mm. controle remoto para regulagem de profundidade do trabalho e transporte por meio do sistema\\nhidraulico e pneus agricolas largura de corte minima de 1500 mm. compativel com tratores de\\npotencia minima de 75 CV; Marca: CIMAG; Modelo / Versao: GRADE ARADORA CIMAG 14 x\\n26 x 6,0, consoante especifica o Edital de Pregao Eletronico no 04/2018 (Doc. Sei id 5507666) e a\\nProposta (Doc. Sei id14269745), que passam a integrar o presente Termo'],\n"," 'vigencia_contrato': ['O contrato tera vigencia desde a sua assinatura\\npor ate 12 meses']}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_extrato_contrato['label'] = labels\n","df_extrato_contrato.label.values[0]"]},{"cell_type":"markdown","metadata":{"id":"68_ldaLCk_kz"},"source":["Transformando dicionários em uma string de anotação..."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1667689472474,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"LkuSX4ZffnyC"},"outputs":[],"source":["df_extrato_contrato['label_txt'] = df_extrato_contrato.label.apply(\n","    lambda x :' # '.join([' # '.join([f'{k}: {v}'for v in vs]) for k,vs in x.items()]))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1667689472475,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"WjjVsyYNkZuD","outputId":"73c9c11a-a4f4-43d2-f3ee-3677fe110c89"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'numero_contrato: 45/2018 # processo_gdf: 00070-00016600/2018-76 # orgao_contratante: SEAGRI/DF # entidade_contratada: CIMAG  COMERCIO DE\\nIMPLEMENTOS E MAQUINAS AGRICOLAS LTDA # valor_contrato: 39.600,00 # nota_empenho: 2018NE00463 # nota_empenho: 2018NE00464 # unidade_orcamentaria: 14101 # programa_trabalho: 20.606.6207.2889.0003 # natureza_despesa: 449052 # fonte_recurso: 732014482 # fonte_recurso: 100000000 # data_assinatura_contrato: 21/11/2018 # objeto_contrato: O Contrato tem por objeto a aquisicao\\ndos seguintes itens: Item 20: Quantidade 03 ( tres). GRADE ARADORA CONTROLE R E M O TO ,\\ncom 14 discos de 26 polegadas de diametro e 06 mm de espessura minima, mancais de rolamento\\ncom lubrificacao permanente em banho de oleo, ou a graxa, espacamento minimo entre os discos de\\n230mm. controle remoto para regulagem de profundidade do trabalho e transporte por meio do sistema\\nhidraulico e pneus agricolas largura de corte minima de 1500 mm. compativel com tratores de\\npotencia minima de 75 CV; Marca: CIMAG; Modelo / Versao: GRADE ARADORA CIMAG 14 x\\n26 x 6,0, consoante especifica o Edital de Pregao Eletronico no 04/2018 (Doc. Sei id 5507666) e a\\nProposta (Doc. Sei id14269745), que passam a integrar o presente Termo # vigencia_contrato: O contrato tera vigencia desde a sua assinatura\\npor ate 12 meses'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_extrato_contrato.label_txt.values[0]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1667689472475,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"QZE9VPCxSmIW","outputId":"989424f4-dc4f-4018-a01c-4fe31f0657be"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-982a676b-4a8b-45ec-ae35-364b51809920\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_ato</th>\n","      <th>texto</th>\n","      <th>label</th>\n","      <th>label_txt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>194</th>\n","      <td>12_22.11.2018-R1</td>\n","      <td>EXTRATO DO CONTRATO PARA AQUISICAO DE BENS No ...</td>\n","      <td>{'numero_contrato': ['45/2018'], 'processo_gdf...</td>\n","      <td>numero_contrato: 45/2018 # processo_gdf: 00070...</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>12_22.11.2018-R2</td>\n","      <td>EXTRATO DO CONTRATO PARA AQUISICAO DE BENS No ...</td>\n","      <td>{'orgao_contratante': ['SEAGRI/DF'], 'entidade...</td>\n","      <td>orgao_contratante: SEAGRI/DF # entidade_contra...</td>\n","    </tr>\n","    <tr>\n","      <th>240</th>\n","      <td>12_22.11.2018-R3</td>\n","      <td>EXTRATO DO CONTRATO No 32/2018\\nAQUISICAO DE B...</td>\n","      <td>{'numero_contrato': ['32/2018'], 'processo_gdf...</td>\n","      <td>numero_contrato: 32/2018 # processo_gdf: 00053...</td>\n","    </tr>\n","    <tr>\n","      <th>297</th>\n","      <td>12_22.11.2018-R4</td>\n","      <td>EXTRATO DE CONTRATO\\nCONTRATO No 9020. ASSINAT...</td>\n","      <td>{'numero_contrato': ['9020'], 'data_assinatura...</td>\n","      <td>numero_contrato: 9020 # data_assinatura_contra...</td>\n","    </tr>\n","    <tr>\n","      <th>371</th>\n","      <td>12_22.11.2018-R7</td>\n","      <td>EXTRATO DO CONTRATO No 01/2018,\\nNOS TERMOS DO...</td>\n","      <td>{'numero_contrato': ['01/2018'], 'processo_gdf...</td>\n","      <td>numero_contrato: 01/2018 # processo_gdf: 00367...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-982a676b-4a8b-45ec-ae35-364b51809920')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-982a676b-4a8b-45ec-ae35-364b51809920 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-982a676b-4a8b-45ec-ae35-364b51809920');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["               id_ato                                              texto  \\\n","194  12_22.11.2018-R1  EXTRATO DO CONTRATO PARA AQUISICAO DE BENS No ...   \n","210  12_22.11.2018-R2  EXTRATO DO CONTRATO PARA AQUISICAO DE BENS No ...   \n","240  12_22.11.2018-R3  EXTRATO DO CONTRATO No 32/2018\\nAQUISICAO DE B...   \n","297  12_22.11.2018-R4  EXTRATO DE CONTRATO\\nCONTRATO No 9020. ASSINAT...   \n","371  12_22.11.2018-R7  EXTRATO DO CONTRATO No 01/2018,\\nNOS TERMOS DO...   \n","\n","                                                 label  \\\n","194  {'numero_contrato': ['45/2018'], 'processo_gdf...   \n","210  {'orgao_contratante': ['SEAGRI/DF'], 'entidade...   \n","240  {'numero_contrato': ['32/2018'], 'processo_gdf...   \n","297  {'numero_contrato': ['9020'], 'data_assinatura...   \n","371  {'numero_contrato': ['01/2018'], 'processo_gdf...   \n","\n","                                             label_txt  \n","194  numero_contrato: 45/2018 # processo_gdf: 00070...  \n","210  orgao_contratante: SEAGRI/DF # entidade_contra...  \n","240  numero_contrato: 32/2018 # processo_gdf: 00053...  \n","297  numero_contrato: 9020 # data_assinatura_contra...  \n","371  numero_contrato: 01/2018 # processo_gdf: 00367...  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df_extrato_contrato[['id_ato', 'texto', 'label', 'label_txt']].head()"]},{"cell_type":"markdown","metadata":{"id":"XsUewAp-mjvv"},"source":["### Treinando GPT"]},{"cell_type":"markdown","metadata":{"id":"3pC5onKxxX2A"},"source":["#### Procurando exemplos para treinamento"]},{"cell_type":"markdown","metadata":{"id":"-jw3fYX5kqBm"},"source":["Não existe ato contendo todas as entidades, sendo assim, é necessário procurar o menor número de atos que tenham exemplos de todas as entidades."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1667689472476,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"UChtCA4Als_9","outputId":"ed1dccae-dbdb-4af4-82f9-d70802981491"},"outputs":[{"name":"stdout","output_type":"stream","text":["cnpj_entidade_contratada\n","cnpj_orgao_contratante\n","codigo_siggo\n","data_assinatura_contrato\n","entidade_contratada\n","fonte_recurso\n","natureza_despesa\n","nome_responsavel\n","nota_empenho\n","numero_contrato\n","objeto_contrato\n","orgao_contratante\n","processo_gdf\n","programa_trabalho\n","unidade_orcamentaria\n","valor_contrato\n","vigencia_contrato\n","--------------------------------------------------\n","17 entidades\n"]}],"source":["entidades_extrato_contrato = set(filter(str.islower, df_extrato_contrato_label.tipo_ent.unique()))\n","print('\\n'.join(sorted(entidades_extrato_contrato)))\n","print('-'*50)\n","print(f'{len(entidades_extrato_contrato)} entidades')"]},{"cell_type":"markdown","metadata":{"id":"GvCp92dOw7m7"},"source":["Organizando os atos por quantidade de entidades:"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1667689472476,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"F5Rp-fXk_m1l","outputId":"911a83d6-de58-4ab1-8cc2-26a71a244b1e"},"outputs":[{"data":{"text/plain":["array([1147, 1396, 1397, ..., 1227, 1422, 1433])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["most_entities_ato = np.argsort([(-len(d)) for d in df_extrato_contrato.label.values])\n","most_entities_ato"]},{"cell_type":"markdown","metadata":{"id":"_PnKH1ncxFUF"},"source":["Selecionando atos para serem utilizados como exemplo, empregando o maior número de entidades possível"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1667689472477,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"WilZHmserZaY"},"outputs":[],"source":["acc = set()\n","example_ids = []\n","for e in most_entities_ato:\n","    if not len(entidades_extrato_contrato - acc):\n","        break\n","    x = set(df_extrato_contrato.label.values[e])\n","    if len(set(x) - acc):\n","        acc |= set(x)\n","        example_ids.append(e)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1667689472477,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"5TfsiLaQ_-Ud","outputId":"eb238910-408d-41ec-b1c5-31ef84db554b"},"outputs":[{"name":"stdout","output_type":"stream","text":["id: 1147\n","15 Entities: {'natureza_despesa', 'unidade_orcamentaria', 'entidade_contratada', 'numero_contrato', 'programa_trabalho', 'data_assinatura_contrato', 'fonte_recurso', 'objeto_contrato', 'nota_empenho', 'processo_gdf', 'orgao_contratante', 'cnpj_entidade_contratada', 'valor_contrato', 'vigencia_contrato', 'cnpj_orgao_contratante'}\n","Missing : {'codigo_siggo', 'nome_responsavel'}\n","--------------------------------------------------\n","id: 646\n","15 Entities: {'unidade_orcamentaria', 'natureza_despesa', 'entidade_contratada', 'numero_contrato', 'programa_trabalho', 'data_assinatura_contrato', 'fonte_recurso', 'objeto_contrato', 'nota_empenho', 'processo_gdf', 'orgao_contratante', 'codigo_siggo', 'cnpj_entidade_contratada', 'valor_contrato', 'vigencia_contrato'}\n","Missing : {'nome_responsavel', 'cnpj_orgao_contratante'}\n","--------------------------------------------------\n","id: 865\n","13 Entities: {'unidade_orcamentaria', 'entidade_contratada', 'numero_contrato', 'programa_trabalho', 'data_assinatura_contrato', 'fonte_recurso', 'objeto_contrato', 'nota_empenho', 'nome_responsavel', 'processo_gdf', 'orgao_contratante', 'valor_contrato', 'vigencia_contrato'}\n","Missing : {'natureza_despesa', 'cnpj_entidade_contratada', 'codigo_siggo', 'cnpj_orgao_contratante'}\n","--------------------------------------------------\n"]}],"source":["for id in example_ids:\n","    ents = df_extrato_contrato.label.values[id]\n","    print('id:', id)\n","    print(f'{len(ents)} Entities:', set(ents))\n","    print('Missing :', entidades_extrato_contrato - set(ents))\n","    print('-'*50)"]},{"cell_type":"markdown","metadata":{"id":"ZniwwhQ5xiRE"},"source":["#### Realizando treinamento com exemplos"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1667689472478,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"AjEOmG4VjIBt","outputId":"419ea344-2fb5-4ab5-cb62-3f5d6f231ead"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'EXTRATO DE CONTRATO PARA AQUISIÇÃO DE BENS Nº 12/2021\\nProcesso: 04011-00001803/2021-37. Partes: O DISTRITO FEDERAL, por meio da SECRETARIA DE ESTADO DA MULHER DO DISTRITO FEDERAL, CNPJ nº 15.169.975/0001-15, e a empresa INDÚSTRIA DE ÁGUA MINERAL IBIÁ LTDA, CNPJ nº 05.655.158/0001-13. Objeto: aquisição de material do gênero alimentício (água potável) e material de acondicionamento e embalagem (garrafão retornável - vasilhame) destinado ao funcionamento desta Secretaria de Estado da Mulher do Distrito Federal. UNIDADE ORÇAMENTÁRIA: 57.101. PROGRAMA DE TRABALHO: 14.122.8211.8517.0163. NATUREZA DA DESPESA: 339030. FONTE DE RECURSO: 100. NOTA DE EMPENHO INICAL: nº 2021NE00159, no valor de R$ 3.186,00 (três mil, cento e oitenta e seis reais), emitida em 24/08/2021. EVENTO: 400091. MODALIDADE: Estimativo. VALOR DO CONTRATO: R$ 11.469,60 (onze mil, quatrocentos e sessenta e nove reais e sessenta centavos). VIGÊNCIA: O contrato terá vigência de 12 (dose) meses, a contar de 03/09/2021 até 03/09/2022. DA ASSINATURA: 24/08/2021. SIGNATÁRIOS: pela Contratante: VANDERCY ANTONIA DE CAMARGOS, na qualidade de Secretária Executiva; pela Contratada: EDUARDO BARROS DE QUEIROZ RODRIGUES, na qualidade de Representante Legal.'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["input_txt = df_extrato_contrato.texto.values[example_ids]\n","input_txt[0]"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1667689472478,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"J8wGImdyiHjm","outputId":"5afc0559-da0f-4181-d378-8494aebc1316"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'fonte_recurso: 100 # natureza_despesa: 339030 # entidade_contratada: INDÚSTRIA DE ÁGUA MINERAL IBIÁ LTDA # cnpj_entidade_contratada: 05.655.158/0001-13 # numero_contrato: 12/2021 # processo_gdf: 04011-00001803/2021-37 # orgao_contratante: SECRETARIA DE ESTADO DA MULHER DO DISTRITO FEDERAL # cnpj_orgao_contratante: 15.169.975/0001-15 # objeto_contrato: aquisição de material do gênero alimentício (água potável) e material de acondicionamento e embalagem (garrafão retornável - vasilhame) destinado ao funcionamento desta Secretaria de Estado da Mulher do Distrito Federal # unidade_orcamentaria: 57.101 # programa_trabalho: 14.122.8211.8517.0163 # nota_empenho: 2021NE00159 # valor_contrato: 11.469,60 # vigencia_contrato: O contrato terá vigência de 12 (dose) meses, a contar de 03/09/2021 até 03/09/2022 # data_assinatura_contrato: 24/08/2021'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["output_txt = df_extrato_contrato.label_txt.values[example_ids]\n","output_txt[0]"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1667689472479,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"JmOgP9LWjpEU"},"outputs":[],"source":["models = [GPT3(engine=\"davinci\", temperature=0, max_tokens=400) for _ in range(len(example_ids))]"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1667689472479,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"JdXBg1sij1l-"},"outputs":[],"source":["for model, i, o in zip(models, input_txt, output_txt):\n","    model.add_example(Example(i, o))"]},{"cell_type":"markdown","metadata":{"id":"IW1JbTVkmqOz"},"source":["### Realizando Predições"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1667689472481,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"-NjlQTLO1g8f"},"outputs":[],"source":["np.random.seed(42)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c2287e3f5e724d6a91045590b829f0f3","8d05929dd4164443b735c00de5993965","0d699c1674e34a0aafab76918700692a","961d8b0b02b64539bf81395ecddc6514","3a80331c91c64a14bea56a08a3cf212b","399486a5863b4814b01abaf02a85fba5","1a7b7e715fce4d41a97f1752a4621a68","d760475a4ec14dfbb9917da2d98c834d","a20ebad9d99c426ba8304b6a5c16db4a","f0ab286028334ca8a62652605a6c8539","eb64c1cac63c4cd9b2ff13fd4b1dbabb"]},"executionInfo":{"elapsed":11365303,"status":"ok","timestamp":1667700837764,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"7IrTl6t2mwFL","outputId":"a76dc6b0-76d0-48b6-9c19-7be1388eb64a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2287e3f5e724d6a91045590b829f0f3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1542 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2144 tokens (1744 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2291 tokens (1891 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2070 tokens (1670 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2198 tokens (1798 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2393 tokens (1993 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2731 tokens (2331 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2253 tokens (1853 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2457 tokens (2057 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2437 tokens (2037 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2300 tokens (1900 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2263 tokens (1863 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2288 tokens (1888 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2277 tokens (1877 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2131 tokens (1731 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2241 tokens (1841 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2303 tokens (1903 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3161 tokens (2761 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2499 tokens (2099 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2456 tokens (2056 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2138 tokens (1738 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2139 tokens (1739 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2157 tokens (1757 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2100 tokens (1700 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2175 tokens (1775 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2455 tokens (2055 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2191 tokens (1791 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2272 tokens (1872 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2240 tokens (1840 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2209 tokens (1809 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2300 tokens (1900 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2078 tokens (1678 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2073 tokens (1673 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2161 tokens (1761 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2236 tokens (1836 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2235 tokens (1835 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2069 tokens (1669 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2278 tokens (1878 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2111 tokens (1711 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2239 tokens (1839 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2210 tokens (1810 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2300 tokens (1900 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2131 tokens (1731 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2141 tokens (1741 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2189 tokens (1789 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2278 tokens (1878 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2397 tokens (1997 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2062 tokens (1662 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2147 tokens (1747 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2339 tokens (1939 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2256 tokens (1856 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2142 tokens (1742 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2353 tokens (1953 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2358 tokens (1958 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2359 tokens (1959 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2372 tokens (1972 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2364 tokens (1964 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2131 tokens (1731 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2307 tokens (1907 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2099 tokens (1699 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2720 tokens (2320 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2147 tokens (1747 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2443 tokens (2043 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2148 tokens (1748 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2103 tokens (1703 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2194 tokens (1794 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2471 tokens (2071 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2152 tokens (1752 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2385 tokens (1985 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2179 tokens (1779 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2385 tokens (1985 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2070 tokens (1670 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2081 tokens (1681 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2418 tokens (2018 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2404 tokens (2004 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2414 tokens (2014 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2400 tokens (2000 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2063 tokens (1663 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2070 tokens (1670 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2089 tokens (1689 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2080 tokens (1680 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2074 tokens (1674 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2397 tokens (1997 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2394 tokens (1994 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2400 tokens (2000 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2410 tokens (2010 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2188 tokens (1788 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2082 tokens (1682 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2083 tokens (1683 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2395 tokens (1995 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2208 tokens (1808 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2083 tokens (1683 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2381 tokens (1981 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2364 tokens (1964 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2403 tokens (2003 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2137 tokens (1737 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2303 tokens (1903 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2454 tokens (2054 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2079 tokens (1679 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2073 tokens (1673 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3020 tokens (2620 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3104 tokens (2704 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2304 tokens (1904 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2111 tokens (1711 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2121 tokens (1721 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2071 tokens (1671 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2075 tokens (1675 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2079 tokens (1679 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2404 tokens (2004 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2222 tokens (1822 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2224 tokens (1824 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2356 tokens (1956 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2051 tokens (1651 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2345 tokens (1945 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2317 tokens (1917 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2402 tokens (2002 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2336 tokens (1936 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2179 tokens (1779 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2100 tokens (1700 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2370 tokens (1970 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2362 tokens (1962 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2363 tokens (1963 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2363 tokens (1963 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2356 tokens (1956 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2351 tokens (1951 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2363 tokens (1963 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2517 tokens (2117 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2152 tokens (1752 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2189 tokens (1789 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2402 tokens (2002 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2439 tokens (2039 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2633 tokens (2233 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2486 tokens (2086 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2163 tokens (1763 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2415 tokens (2015 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2405 tokens (2005 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2213 tokens (1813 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2164 tokens (1764 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2205 tokens (1805 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2378 tokens (1978 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2754 tokens (2354 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2363 tokens (1963 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2105 tokens (1705 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 4324 tokens (3924 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2498 tokens (2098 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2263 tokens (1863 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2584 tokens (2184 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2129 tokens (1729 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2434 tokens (2034 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2128 tokens (1728 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2167 tokens (1767 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2241 tokens (1841 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2203 tokens (1803 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2385 tokens (1985 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2403 tokens (2003 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2183 tokens (1783 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2237 tokens (1837 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2692 tokens (2292 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2168 tokens (1768 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2470 tokens (2070 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2137 tokens (1737 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2466 tokens (2066 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2352 tokens (1952 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2286 tokens (1886 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2386 tokens (1986 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2361 tokens (1961 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2169 tokens (1769 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2238 tokens (1838 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2447 tokens (2047 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2070 tokens (1670 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2376 tokens (1976 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2388 tokens (1988 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2064 tokens (1664 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2063 tokens (1663 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2373 tokens (1973 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2345 tokens (1945 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2387 tokens (1987 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2053 tokens (1653 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2816 tokens (2416 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2200 tokens (1800 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2267 tokens (1867 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2349 tokens (1949 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2350 tokens (1950 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2074 tokens (1674 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2166 tokens (1766 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2131 tokens (1731 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2278 tokens (1878 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2183 tokens (1783 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2353 tokens (1953 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2341 tokens (1941 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2336 tokens (1936 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2629 tokens (2229 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2400 tokens (2000 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3624 tokens (3224 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3274 tokens (2874 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2578 tokens (2178 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3376 tokens (2976 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2413 tokens (2013 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2406 tokens (2006 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2186 tokens (1786 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2085 tokens (1685 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2671 tokens (2271 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2110 tokens (1710 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2397 tokens (1997 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2367 tokens (1967 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2671 tokens (2271 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2378 tokens (1978 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2067 tokens (1667 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2228 tokens (1828 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2354 tokens (1954 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2456 tokens (2056 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2445 tokens (2045 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2401 tokens (2001 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2240 tokens (1840 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2524 tokens (2124 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2093 tokens (1693 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2063 tokens (1663 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2091 tokens (1691 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2072 tokens (1672 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2368 tokens (1968 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2172 tokens (1772 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2193 tokens (1793 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2229 tokens (1829 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2180 tokens (1780 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2293 tokens (1893 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2359 tokens (1959 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2527 tokens (2127 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2535 tokens (2135 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3056 tokens (2656 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2129 tokens (1729 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 4000 tokens (3600 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2130 tokens (1730 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2137 tokens (1737 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2135 tokens (1735 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2098 tokens (1698 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2163 tokens (1763 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2417 tokens (2017 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2393 tokens (1993 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2179 tokens (1779 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3432 tokens (3032 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2219 tokens (1819 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2380 tokens (1980 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2517 tokens (2117 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2356 tokens (1956 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2152 tokens (1752 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2289 tokens (1889 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2418 tokens (2018 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2074 tokens (1674 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2258 tokens (1858 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2061 tokens (1661 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2128 tokens (1728 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2201 tokens (1801 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2216 tokens (1816 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2106 tokens (1706 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2289 tokens (1889 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2261 tokens (1861 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2217 tokens (1817 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2082 tokens (1682 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2637 tokens (2237 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2882 tokens (2482 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2259 tokens (1859 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2256 tokens (1856 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2130 tokens (1730 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2430 tokens (2030 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2254 tokens (1854 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2088 tokens (1688 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2297 tokens (1897 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2201 tokens (1801 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2271 tokens (1871 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2469 tokens (2069 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2244 tokens (1844 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2160 tokens (1760 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2464 tokens (2064 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2488 tokens (2088 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2165 tokens (1765 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2156 tokens (1756 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2258 tokens (1858 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2358 tokens (1958 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2269 tokens (1869 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2265 tokens (1865 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2076 tokens (1676 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2265 tokens (1865 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2316 tokens (1916 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2319 tokens (1919 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2389 tokens (1989 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2074 tokens (1674 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2214 tokens (1814 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2449 tokens (2049 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2562 tokens (2162 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2663 tokens (2263 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2532 tokens (2132 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2238 tokens (1838 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2172 tokens (1772 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2153 tokens (1753 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2830 tokens (2430 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2289 tokens (1889 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2423 tokens (2023 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3689 tokens (3289 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2630 tokens (2230 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3031 tokens (2631 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3030 tokens (2630 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2865 tokens (2465 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2679 tokens (2279 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2509 tokens (2109 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2344 tokens (1944 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2111 tokens (1711 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2076 tokens (1676 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2264 tokens (1864 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2331 tokens (1931 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2252 tokens (1852 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2245 tokens (1845 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2451 tokens (2051 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2488 tokens (2088 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2264 tokens (1864 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2566 tokens (2166 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2555 tokens (2155 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2688 tokens (2288 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2095 tokens (1695 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2590 tokens (2190 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3898 tokens (3498 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2719 tokens (2319 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2290 tokens (1890 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2056 tokens (1656 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2074 tokens (1674 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2423 tokens (2023 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2076 tokens (1676 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2406 tokens (2006 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2438 tokens (2038 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2452 tokens (2052 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2246 tokens (1846 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2396 tokens (1996 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2413 tokens (2013 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2221 tokens (1821 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2094 tokens (1694 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2090 tokens (1690 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2103 tokens (1703 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2110 tokens (1710 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2291 tokens (1891 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2110 tokens (1710 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2408 tokens (2008 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2066 tokens (1666 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2066 tokens (1666 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2399 tokens (1999 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2355 tokens (1955 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2405 tokens (2005 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2394 tokens (1994 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2411 tokens (2011 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2392 tokens (1992 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2385 tokens (1985 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2496 tokens (2096 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2552 tokens (2152 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2052 tokens (1652 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2069 tokens (1669 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2382 tokens (1982 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2409 tokens (2009 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2183 tokens (1783 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2539 tokens (2139 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2772 tokens (2372 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2238 tokens (1838 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2623 tokens (2223 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2071 tokens (1671 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2360 tokens (1960 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2353 tokens (1953 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2362 tokens (1962 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2056 tokens (1656 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2347 tokens (1947 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2345 tokens (1945 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2356 tokens (1956 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2351 tokens (1951 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2081 tokens (1681 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2093 tokens (1693 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2058 tokens (1658 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2121 tokens (1721 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2156 tokens (1756 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2391 tokens (1991 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2546 tokens (2146 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2134 tokens (1734 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2055 tokens (1655 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2077 tokens (1677 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2731 tokens (2331 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2102 tokens (1702 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2441 tokens (2041 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2463 tokens (2063 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2118 tokens (1718 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2517 tokens (2117 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2227 tokens (1827 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2256 tokens (1856 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2196 tokens (1796 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2158 tokens (1758 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2153 tokens (1753 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2157 tokens (1757 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2147 tokens (1747 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2156 tokens (1756 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2143 tokens (1743 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2152 tokens (1752 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2144 tokens (1744 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2148 tokens (1748 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2144 tokens (1744 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2144 tokens (1744 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2153 tokens (1753 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2149 tokens (1749 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2148 tokens (1748 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2156 tokens (1756 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2150 tokens (1750 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2144 tokens (1744 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2159 tokens (1759 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2158 tokens (1758 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2155 tokens (1755 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2147 tokens (1747 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2151 tokens (1751 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2145 tokens (1745 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2146 tokens (1746 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2141 tokens (1741 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2215 tokens (1815 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2269 tokens (1869 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2181 tokens (1781 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2148 tokens (1748 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2296 tokens (1896 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2291 tokens (1891 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2437 tokens (2037 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2403 tokens (2003 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2067 tokens (1667 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2714 tokens (2314 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2221 tokens (1821 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2608 tokens (2208 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2125 tokens (1725 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2099 tokens (1699 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2120 tokens (1720 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2857 tokens (2457 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2237 tokens (1837 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2349 tokens (1949 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2452 tokens (2052 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2318 tokens (1918 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2321 tokens (1921 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2281 tokens (1881 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2368 tokens (1968 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2172 tokens (1772 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2248 tokens (1848 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2061 tokens (1661 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2064 tokens (1664 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2065 tokens (1665 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2417 tokens (2017 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2113 tokens (1713 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2448 tokens (2048 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2182 tokens (1782 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2136 tokens (1736 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2217 tokens (1817 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2087 tokens (1687 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2197 tokens (1797 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2202 tokens (1802 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2495 tokens (2095 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2406 tokens (2006 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2270 tokens (1870 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2186 tokens (1786 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2328 tokens (1928 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2496 tokens (2096 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2598 tokens (2198 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2171 tokens (1771 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2065 tokens (1665 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2528 tokens (2128 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2472 tokens (2072 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2653 tokens (2253 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2635 tokens (2235 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2063 tokens (1663 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2082 tokens (1682 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2133 tokens (1733 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2588 tokens (2188 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2507 tokens (2107 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2374 tokens (1974 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2101 tokens (1701 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2234 tokens (1834 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2134 tokens (1734 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2361 tokens (1961 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2185 tokens (1785 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2444 tokens (2044 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2900 tokens (2500 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2069 tokens (1669 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2340 tokens (1940 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2128 tokens (1728 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2508 tokens (2108 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2552 tokens (2152 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2330 tokens (1930 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2413 tokens (2013 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2382 tokens (1982 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2083 tokens (1683 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2115 tokens (1715 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2052 tokens (1652 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2504 tokens (2104 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2082 tokens (1682 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2229 tokens (1829 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2160 tokens (1760 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2381 tokens (1981 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2154 tokens (1754 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2052 tokens (1652 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2256 tokens (1856 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2314 tokens (1914 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2309 tokens (1909 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2058 tokens (1658 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2338 tokens (1938 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2444 tokens (2044 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2205 tokens (1805 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2595 tokens (2195 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2123 tokens (1723 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2179 tokens (1779 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2671 tokens (2271 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2766 tokens (2366 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 3124 tokens (2724 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2241 tokens (1841 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2830 tokens (2430 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2405 tokens (2005 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2343 tokens (1943 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2419 tokens (2019 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2351 tokens (1951 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2458 tokens (2058 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2361 tokens (1961 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2356 tokens (1956 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2259 tokens (1859 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2357 tokens (1957 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2462 tokens (2062 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2357 tokens (1957 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2052 tokens (1652 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2169 tokens (1769 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2666 tokens (2266 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2666 tokens (2266 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2169 tokens (1769 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2347 tokens (1947 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2169 tokens (1769 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2347 tokens (1947 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2666 tokens (2266 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2347 tokens (1947 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2666 tokens (2266 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2122 tokens (1722 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2122 tokens (1722 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2122 tokens (1722 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2441 tokens (2041 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2441 tokens (2041 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2441 tokens (2041 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2196 tokens (1796 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2196 tokens (1796 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2515 tokens (2115 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2196 tokens (1796 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2068 tokens (1668 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2199 tokens (1799 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2518 tokens (2118 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2518 tokens (2118 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2199 tokens (1799 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2518 tokens (2118 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2064 tokens (1664 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2152 tokens (1752 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2479 tokens (2079 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2452 tokens (2052 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2302 tokens (1902 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2349 tokens (1949 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2166 tokens (1766 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2372 tokens (1972 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2213 tokens (1813 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2982 tokens (2582 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2089 tokens (1689 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2339 tokens (1939 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2357 tokens (1957 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2059 tokens (1659 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2384 tokens (1984 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2054 tokens (1654 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2389 tokens (1989 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2289 tokens (1889 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2290 tokens (1890 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2291 tokens (1891 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2058 tokens (1658 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2141 tokens (1741 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2228 tokens (1828 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2140 tokens (1740 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2235 tokens (1835 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2235 tokens (1835 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2113 tokens (1713 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2192 tokens (1792 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2293 tokens (1893 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2340 tokens (1940 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2206 tokens (1806 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2255 tokens (1855 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2139 tokens (1739 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2277 tokens (1877 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2318 tokens (1918 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 0\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2104 tokens (1704 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2409 tokens (2009 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2127 tokens (1727 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2365 tokens (1965 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2093 tokens (1693 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2132 tokens (1732 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2166 tokens (1766 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2143 tokens (1743 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2190 tokens (1790 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2368 tokens (1968 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2062 tokens (1662 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2164 tokens (1764 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2189 tokens (1789 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 1\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2520 tokens (2120 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","Modelo: 2\n","Erro:This model's maximum context length is 2049 tokens, however you requested 2086 tokens (1686 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.\n","--------------------------------------------------\n","Numero total de tokens usadas 1565108\n","preco final: 31.30216\n"]}],"source":["from tqdm.notebook import tqdm\n","\n","cont_tokens = 0\n","preds = []\n","for text in tqdm(df_extrato_contrato.texto):\n","    n = np.random.randint(0, len(example_ids))\n","    try:\n","        response =  models[n].submit_request(text)\n","        pred = (response.to_dict()[\"choices\"][0][\"text\"].replace(\"output: \", ''))\n","        preds.append(pred)\n","        # contador de tokens:\n","        cont_tokens += int(response.to_dict()['usage']['total_tokens'])\n","    except Exception as e:\n","        print(f'Modelo: {n}')\n","        print(f'Erro:{e}')\n","        preds.append('')\n","print('-'*50)        \n","print(\"Numero total de tokens usadas\",cont_tokens)\n","print(\"preco final:\",(cont_tokens/1000)*0.02)"]},{"cell_type":"markdown","metadata":{"id":"F1UY3w3Hzv-L"},"source":["Transformando predições em dicionários:"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1667703064473,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"-hZsR65X71LH"},"outputs":[],"source":["dict_preds = []\n","for pred in preds:\n","    dict_pred = {}\n","    list_pred = [(k.strip(),v.strip()) for k, _, v in  [x.partition(':') for x in pred.split('#')] if k!=v!='']\n","    for k, v in list_pred:\n","        if k not in dict_pred:\n","            if k in entidades_extrato_contrato:\n","                dict_pred[k] = [v]\n","        else:\n","            dict_pred[k].append(v)\n","    dict_preds.append(dict_pred)"]},{"cell_type":"markdown","metadata":{"id":"MiUuFdn85UxX"},"source":["Contando quantidade de predições feitas de fato:"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1667703437737,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"wmfpuK_Yx08K","outputId":"10f5523b-bec4-4f2d-f1a3-c64d0a37f429"},"outputs":[{"name":"stdout","output_type":"stream","text":["759 predições de 1542 atos\n"]}],"source":["n_errors = len([x for x in dict_preds if x!={}])\n","print(f'{len(preds) - n_errors} predições de {len(preds)} atos')"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":245,"status":"ok","timestamp":1667703559034,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"kZKkEi6F4GIX"},"outputs":[],"source":["df_extrato_contrato['pred'] = dict_preds"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1667703623147,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"NzepaCRr4Nns","outputId":"475ef9f3-ece2-4a16-9325-09850736b80e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-32f117c1-0c9e-4034-b8a6-c667c804d0c0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>194</th>\n","      <td>{'numero_contrato': ['45/2018'], 'processo_gdf...</td>\n","      <td>{}</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>{'orgao_contratante': ['SEAGRI/DF'], 'entidade...</td>\n","      <td>{'fonte_recurso': ['332012027'], 'natureza_des...</td>\n","    </tr>\n","    <tr>\n","      <th>240</th>\n","      <td>{'numero_contrato': ['32/2018'], 'processo_gdf...</td>\n","      <td>{'orgao_contratante': ['INSTITUTO DE ASSIST. À...</td>\n","    </tr>\n","    <tr>\n","      <th>297</th>\n","      <td>{'numero_contrato': ['9020'], 'data_assinatura...</td>\n","      <td>{}</td>\n","    </tr>\n","    <tr>\n","      <th>371</th>\n","      <td>{'numero_contrato': ['01/2018'], 'processo_gdf...</td>\n","      <td>{'fonte_recurso': ['100'], 'natureza_despesa':...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>40914</th>\n","      <td>{'data_assinatura_contrato': ['15/03/2022'], '...</td>\n","      <td>{'orgao_contratante': ['SEEDF', 'SEEDF'], 'obj...</td>\n","    </tr>\n","    <tr>\n","      <th>40924</th>\n","      <td>{'orgao_contratante': ['SEEDF'], 'unidade_orca...</td>\n","      <td>{}</td>\n","    </tr>\n","    <tr>\n","      <th>40994</th>\n","      <td>{'fonte_recurso': ['100'], 'data_assinatura_co...</td>\n","      <td>{}</td>\n","    </tr>\n","    <tr>\n","      <th>41008</th>\n","      <td>{'fonte_recurso': ['100'], 'data_assinatura_co...</td>\n","      <td>{'fonte_recurso': ['100'], 'natureza_despesa':...</td>\n","    </tr>\n","    <tr>\n","      <th>41023</th>\n","      <td>{'fonte_recurso': ['100'], 'data_assinatura_co...</td>\n","      <td>{}</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1542 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32f117c1-0c9e-4034-b8a6-c667c804d0c0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-32f117c1-0c9e-4034-b8a6-c667c804d0c0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-32f117c1-0c9e-4034-b8a6-c667c804d0c0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   label  \\\n","194    {'numero_contrato': ['45/2018'], 'processo_gdf...   \n","210    {'orgao_contratante': ['SEAGRI/DF'], 'entidade...   \n","240    {'numero_contrato': ['32/2018'], 'processo_gdf...   \n","297    {'numero_contrato': ['9020'], 'data_assinatura...   \n","371    {'numero_contrato': ['01/2018'], 'processo_gdf...   \n","...                                                  ...   \n","40914  {'data_assinatura_contrato': ['15/03/2022'], '...   \n","40924  {'orgao_contratante': ['SEEDF'], 'unidade_orca...   \n","40994  {'fonte_recurso': ['100'], 'data_assinatura_co...   \n","41008  {'fonte_recurso': ['100'], 'data_assinatura_co...   \n","41023  {'fonte_recurso': ['100'], 'data_assinatura_co...   \n","\n","                                                    pred  \n","194                                                   {}  \n","210    {'fonte_recurso': ['332012027'], 'natureza_des...  \n","240    {'orgao_contratante': ['INSTITUTO DE ASSIST. À...  \n","297                                                   {}  \n","371    {'fonte_recurso': ['100'], 'natureza_despesa':...  \n","...                                                  ...  \n","40914  {'orgao_contratante': ['SEEDF', 'SEEDF'], 'obj...  \n","40924                                                 {}  \n","40994                                                 {}  \n","41008  {'fonte_recurso': ['100'], 'natureza_despesa':...  \n","41023                                                 {}  \n","\n","[1542 rows x 2 columns]"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["df_extrato_contrato[['label', 'pred']]"]},{"cell_type":"markdown","metadata":{"id":"W07tJMRo5fAp"},"source":["Salvando resultados..."]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1667703698322,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"XL3fB2zOAakb"},"outputs":[],"source":["import pickle as pkl\n","\n","with open('drive/MyDrive/Datasets/preds_extrato_contrato.pkl', 'wb') as f:\n","    pkl.dump(preds, f)\n","\n","with open('drive/MyDrive/Datasets/dict_preds_extrato_contrato.pkl', 'wb') as f:\n","    pkl.dump(dict_preds, f)\n","\n","with open('drive/MyDrive/Datasets/df_extrato_contrato.pkl', 'wb') as f:\n","    pkl.dump(df_extrato_contrato, f)"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":279,"status":"ok","timestamp":1667703742515,"user":{"displayName":"Gabriel da Silva Corvino Nogueira","userId":"07665767357131658693"},"user_tz":180},"id":"1X_mcOgm4vyn"},"outputs":[],"source":["df_extrato_contrato.to_csv('drive/MyDrive/Datasets/df_extrato_contrato.csv')"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1bhU8ykA-sht4KSzB47X7IWG5ZrNLaeMX","timestamp":1664823443096},{"file_id":"https://github.com/kes76963/myproject/blob/main/gpt3_edit.ipynb","timestamp":1664310948941}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d699c1674e34a0aafab76918700692a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d760475a4ec14dfbb9917da2d98c834d","max":1542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a20ebad9d99c426ba8304b6a5c16db4a","value":1542}},"1a7b7e715fce4d41a97f1752a4621a68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"399486a5863b4814b01abaf02a85fba5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a80331c91c64a14bea56a08a3cf212b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d05929dd4164443b735c00de5993965":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_399486a5863b4814b01abaf02a85fba5","placeholder":"​","style":"IPY_MODEL_1a7b7e715fce4d41a97f1752a4621a68","value":"100%"}},"961d8b0b02b64539bf81395ecddc6514":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0ab286028334ca8a62652605a6c8539","placeholder":"​","style":"IPY_MODEL_eb64c1cac63c4cd9b2ff13fd4b1dbabb","value":" 1542/1542 [3:09:24&lt;00:00,  4.62s/it]"}},"a20ebad9d99c426ba8304b6a5c16db4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2287e3f5e724d6a91045590b829f0f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d05929dd4164443b735c00de5993965","IPY_MODEL_0d699c1674e34a0aafab76918700692a","IPY_MODEL_961d8b0b02b64539bf81395ecddc6514"],"layout":"IPY_MODEL_3a80331c91c64a14bea56a08a3cf212b"}},"d760475a4ec14dfbb9917da2d98c834d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb64c1cac63c4cd9b2ff13fd4b1dbabb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0ab286028334ca8a62652605a6c8539":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
